{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Jupyter notebook to distignish Cats and Dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import cv2                 # working with, mainly resizing, images\n",
    "import numpy as np         # dealing with arrays\n",
    "import os                  # dealing with directories\n",
    "from random import shuffle # mixing up or currently ordered data that might lead our network astray in training.\n",
    "from tqdm import tqdm      # a nice pretty percentage bar for tasks. Thanks to viewer Daniel BA1/4hler for this suggestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DIR = 'train'\n",
    "TEST_DIR = 'test'\n",
    "IMG_SIZE = 50\n",
    "LR = 1e-3\n",
    "\n",
    "print(\"done loading\")\n",
    "\n",
    "MODEL_NAME = 'dogsvscats-{}-{}.model'.format(LR, '2conv-basic') # just so we remember which saved model is which, sizes must match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_img(img):\n",
    "    word_label = img.split('.')[-3]\n",
    "    # conversion to one-hot array [cat,dog]\n",
    "    #                            [much cat, no dog]\n",
    "    if word_label == 'cat': return [1,0]\n",
    "    #                             [no cat, very doggo]\n",
    "    elif word_label == 'dog': return [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_train_data():\n",
    "    training_data = []\n",
    "    for img in tqdm(os.listdir(TRAIN_DIR)):\n",
    "        if img[-4:] == \".jpg\":\n",
    "            label = label_img(img)\n",
    "            path = os.path.join(TRAIN_DIR,img)\n",
    "            img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "            training_data.append([np.array(img),np.array(label)])\n",
    "    shuffle(training_data)\n",
    "    np.save('train_data.npy', training_data)\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_test_data():\n",
    "    testing_data = []\n",
    "    for img in tqdm(os.listdir(TEST_DIR)):\n",
    "        path = os.path.join(TEST_DIR,img)\n",
    "        img_num = img.split('.')[0]\n",
    "        img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "        testing_data.append([np.array(img), img_num])\n",
    "        \n",
    "    shuffle(testing_data)\n",
    "    np.save('test_data.npy', testing_data)\n",
    "    print(saved)\n",
    "    return testing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25001/25001 [00:37<00:00, 672.15it/s]\n"
     ]
    }
   ],
   "source": [
    "#train_data = create_train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading trainig data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = np.load('train_data.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_net_image_input(image_shape):\n",
    "    return tf.placeholder(tf.float32, shape=[None, image_shape[0], image_shape[1], image_shape[2]], name=\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_net_label_input(n_classes):\n",
    "    return tf.placeholder(tf.float32, shape=[None, n_classes], name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_net_keep_prob_input():\n",
    "    return tf.placeholder(tf.float32, name=\"keep_prob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    depth = int(x_tensor.shape[3])\n",
    "    filter_weights = tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1],depth, conv_num_outputs], stddev=0.1))\n",
    "    filter_bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    stridesTuple = [1, conv_strides[0], conv_strides[1], 1] # (batch, height, width, depth)\n",
    "    paddingString = 'SAME'\n",
    "    conv = tf.nn.conv2d(x_tensor, filter_weights, strides=stridesTuple, padding=paddingString)\n",
    "    conv = tf.nn.bias_add(conv, filter_bias)\n",
    "    conv = tf.nn.relu(conv)\n",
    "    poolTuple = [1, pool_strides[0], pool_strides[1], 1] # (batch, height, width, depth)\n",
    "    conv = tf.nn.max_pool(conv,ksize=[1, pool_ksize[0], pool_ksize[1], 1],strides=poolTuple,padding=paddingString)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(x_tensor):\n",
    "    return tf.contrib.layers.flatten(x_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    return tf.contrib.layers.fully_connected(inputs=x_tensor, num_outputs=num_outputs, activation_fn=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    return tf.contrib.layers.fully_connected(inputs=x_tensor, num_outputs=num_outputs, activation_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    conv = conv2d_maxpool(x, 16, (8,8), (1,1), (2,2), (2,2))\n",
    "    conv = conv2d_maxpool(x, 32, (4,4), (1,1), (2,2), (2,2))\n",
    "    conv = conv2d_maxpool(x, 64, (2,2), (1,1), (2,2), (2,2))\n",
    "\n",
    "    conv = flatten(conv)\n",
    "    \n",
    "    conv = fully_conn(conv, 200)\n",
    "    conv = tf.nn.dropout(conv, keep_prob)\n",
    "    conv = fully_conn(conv, 100)\n",
    "    conv = tf.nn.dropout(conv, keep_prob)\n",
    "    \n",
    "\n",
    "    conv = output(conv, 2)\n",
    "    \n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((IMG_SIZE, IMG_SIZE, 1))\n",
    "y = neural_net_label_input(2)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    output = session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    loss = session.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.0})\n",
    "    #valid_acc = session.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.0})\n",
    "    print('Loss: {}'.format(loss))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 10\n",
    "keep_probability = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train_data[:-500]\n",
    "test = train_data[-500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = np.array([i[0] for i in train]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "train_y = [i[1] for i in train]\n",
    "\n",
    "test_x = np.array([i[0] for i in test]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "test_y = [i[1] for i in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chunks(x, y, n):\n",
    "    for i in range(0, len(x), n):\n",
    "        yield x[i:i + n], y[i:i + n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batches = chunks(train_x, train_y, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, CatsAndDogs Batch 1:  Loss: 72.2221450805664\n",
      "Epoch  1, CatsAndDogs Batch 2:  Loss: 82.20819854736328\n",
      "Epoch  1, CatsAndDogs Batch 3:  Loss: 93.59293365478516\n",
      "Epoch  1, CatsAndDogs Batch 4:  Loss: 107.04024505615234\n",
      "Epoch  1, CatsAndDogs Batch 5:  Loss: 8.2174711227417\n",
      "Epoch  1, CatsAndDogs Batch 6:  Loss: 135.21475219726562\n",
      "Epoch  1, CatsAndDogs Batch 7:  Loss: 165.41964721679688\n",
      "Epoch  1, CatsAndDogs Batch 8:  Loss: 172.78347778320312\n",
      "Epoch  1, CatsAndDogs Batch 9:  Loss: 269.3772888183594\n",
      "Epoch  1, CatsAndDogs Batch 10:  Loss: 176.17453002929688\n",
      "Epoch  1, CatsAndDogs Batch 11:  Loss: 61.896522521972656\n",
      "Epoch  1, CatsAndDogs Batch 12:  Loss: 36.64923095703125\n",
      "Epoch  1, CatsAndDogs Batch 13:  Loss: 171.79896545410156\n",
      "Epoch  1, CatsAndDogs Batch 14:  Loss: 102.31810760498047\n",
      "Epoch  1, CatsAndDogs Batch 15:  Loss: 165.47946166992188\n",
      "Epoch  1, CatsAndDogs Batch 16:  Loss: 155.70156860351562\n",
      "Epoch  1, CatsAndDogs Batch 17:  Loss: 195.67755126953125\n",
      "Epoch  1, CatsAndDogs Batch 18:  Loss: 75.56515502929688\n",
      "Epoch  1, CatsAndDogs Batch 19:  Loss: 11.725625991821289\n",
      "Epoch  1, CatsAndDogs Batch 20:  Loss: 46.107486724853516\n",
      "Epoch  1, CatsAndDogs Batch 21:  Loss: 75.20381927490234\n",
      "Epoch  1, CatsAndDogs Batch 22:  Loss: 59.08945846557617\n",
      "Epoch  1, CatsAndDogs Batch 23:  Loss: 57.30327224731445\n",
      "Epoch  1, CatsAndDogs Batch 24:  Loss: 73.07568359375\n",
      "Epoch  1, CatsAndDogs Batch 25:  Loss: 48.20536422729492\n",
      "Epoch  1, CatsAndDogs Batch 26:  Loss: 59.97507858276367\n",
      "Epoch  1, CatsAndDogs Batch 27:  Loss: 43.508323669433594\n",
      "Epoch  1, CatsAndDogs Batch 28:  Loss: 13.492673873901367\n",
      "Epoch  1, CatsAndDogs Batch 29:  Loss: 12.218263626098633\n",
      "Epoch  1, CatsAndDogs Batch 30:  Loss: 36.058204650878906\n",
      "Epoch  1, CatsAndDogs Batch 31:  Loss: 59.90485763549805\n",
      "Epoch  1, CatsAndDogs Batch 32:  Loss: 34.435081481933594\n",
      "Epoch  1, CatsAndDogs Batch 33:  Loss: 23.81207275390625\n",
      "Epoch  1, CatsAndDogs Batch 34:  Loss: 53.648094177246094\n",
      "Epoch  1, CatsAndDogs Batch 35:  Loss: 23.311389923095703\n",
      "Epoch  1, CatsAndDogs Batch 36:  Loss: 0.46153563261032104\n",
      "Epoch  1, CatsAndDogs Batch 37:  Loss: 7.977025032043457\n",
      "Epoch  1, CatsAndDogs Batch 38:  Loss: 19.821386337280273\n",
      "Epoch  1, CatsAndDogs Batch 39:  Loss: 21.37429428100586\n",
      "Epoch  1, CatsAndDogs Batch 40:  Loss: 22.035064697265625\n",
      "Epoch  1, CatsAndDogs Batch 41:  Loss: 26.825231552124023\n",
      "Epoch  1, CatsAndDogs Batch 42:  Loss: 20.222625732421875\n",
      "Epoch  1, CatsAndDogs Batch 43:  Loss: 10.038753509521484\n",
      "Epoch  1, CatsAndDogs Batch 44:  Loss: 10.934967041015625\n",
      "Epoch  1, CatsAndDogs Batch 45:  Loss: 2.7493128776550293\n",
      "Epoch  1, CatsAndDogs Batch 46:  Loss: 15.258657455444336\n",
      "Epoch  1, CatsAndDogs Batch 47:  Loss: 13.09961986541748\n",
      "Epoch  1, CatsAndDogs Batch 48:  Loss: 14.896188735961914\n",
      "Epoch  1, CatsAndDogs Batch 49:  Loss: 15.023099899291992\n",
      "Epoch  1, CatsAndDogs Batch 50:  Loss: 17.215116500854492\n",
      "Epoch  1, CatsAndDogs Batch 51:  Loss: 9.52135944366455\n",
      "Epoch  1, CatsAndDogs Batch 52:  Loss: 9.054197311401367\n",
      "Epoch  1, CatsAndDogs Batch 53:  Loss: 3.0075433254241943\n",
      "Epoch  1, CatsAndDogs Batch 54:  Loss: 2.0191357135772705\n",
      "Epoch  1, CatsAndDogs Batch 55:  Loss: 1.9068866968154907\n",
      "Epoch  1, CatsAndDogs Batch 56:  Loss: 4.513855934143066\n",
      "Epoch  1, CatsAndDogs Batch 57:  Loss: 1.3353855609893799\n",
      "Epoch  1, CatsAndDogs Batch 58:  Loss: 5.195599555969238\n",
      "Epoch  1, CatsAndDogs Batch 59:  Loss: 3.2711539268493652\n",
      "Epoch  1, CatsAndDogs Batch 60:  Loss: 2.3158931732177734\n",
      "Epoch  1, CatsAndDogs Batch 61:  Loss: 7.260983467102051\n",
      "Epoch  1, CatsAndDogs Batch 62:  Loss: 5.886572360992432\n",
      "Epoch  1, CatsAndDogs Batch 63:  Loss: 2.619807720184326\n",
      "Epoch  1, CatsAndDogs Batch 64:  Loss: 1.9634854793548584\n",
      "Epoch  1, CatsAndDogs Batch 65:  Loss: 1.6602904796600342\n",
      "Epoch  1, CatsAndDogs Batch 66:  Loss: 3.220625638961792\n",
      "Epoch  1, CatsAndDogs Batch 67:  Loss: 6.866143703460693\n",
      "Epoch  1, CatsAndDogs Batch 68:  Loss: 8.44989013671875\n",
      "Epoch  1, CatsAndDogs Batch 69:  Loss: 10.714607238769531\n",
      "Epoch  1, CatsAndDogs Batch 70:  Loss: 4.2930498123168945\n",
      "Epoch  1, CatsAndDogs Batch 71:  Loss: 5.215956687927246\n",
      "Epoch  1, CatsAndDogs Batch 72:  Loss: 10.342309951782227\n",
      "Epoch  1, CatsAndDogs Batch 73:  Loss: 8.955389022827148\n",
      "Epoch  1, CatsAndDogs Batch 74:  Loss: 0.8289176225662231\n",
      "Epoch  1, CatsAndDogs Batch 75:  Loss: 2.9986653327941895\n",
      "Epoch  1, CatsAndDogs Batch 76:  Loss: 2.2541937828063965\n",
      "Epoch  1, CatsAndDogs Batch 77:  Loss: 1.71005380153656\n",
      "Epoch  1, CatsAndDogs Batch 78:  Loss: 1.5943632125854492\n",
      "Epoch  1, CatsAndDogs Batch 79:  Loss: 2.4361116886138916\n",
      "Epoch  1, CatsAndDogs Batch 80:  Loss: 2.7039644718170166\n",
      "Epoch  1, CatsAndDogs Batch 81:  Loss: 2.8606348037719727\n",
      "Epoch  1, CatsAndDogs Batch 82:  Loss: 3.7807259559631348\n",
      "Epoch  1, CatsAndDogs Batch 83:  Loss: 4.058957099914551\n",
      "Epoch  1, CatsAndDogs Batch 84:  Loss: 5.601466178894043\n",
      "Epoch  1, CatsAndDogs Batch 85:  Loss: 1.7817522287368774\n",
      "Epoch  1, CatsAndDogs Batch 86:  Loss: 1.9403979778289795\n",
      "Epoch  1, CatsAndDogs Batch 87:  Loss: 2.1831626892089844\n",
      "Epoch  1, CatsAndDogs Batch 88:  Loss: 1.0435901880264282\n",
      "Epoch  1, CatsAndDogs Batch 89:  Loss: 0.6149482131004333\n",
      "Epoch  1, CatsAndDogs Batch 90:  Loss: 0.9922853708267212\n",
      "Epoch  1, CatsAndDogs Batch 91:  Loss: 0.7638192772865295\n",
      "Epoch  1, CatsAndDogs Batch 92:  Loss: 0.5728589296340942\n",
      "Epoch  1, CatsAndDogs Batch 93:  Loss: 1.5083798170089722\n",
      "Epoch  1, CatsAndDogs Batch 94:  Loss: 0.8267432451248169\n",
      "Epoch  1, CatsAndDogs Batch 95:  Loss: 1.6853501796722412\n",
      "Epoch  1, CatsAndDogs Batch 96:  Loss: 2.7929556369781494\n",
      "Epoch  1, CatsAndDogs Batch 97:  Loss: 1.6757285594940186\n",
      "Epoch  1, CatsAndDogs Batch 98:  Loss: 0.7596642374992371\n",
      "Epoch  1, CatsAndDogs Batch 99:  Loss: 1.6541614532470703\n",
      "Epoch  1, CatsAndDogs Batch 100:  Loss: 1.3660691976547241\n",
      "Epoch  1, CatsAndDogs Batch 101:  Loss: 1.0883586406707764\n",
      "Epoch  1, CatsAndDogs Batch 102:  Loss: 0.6692899465560913\n",
      "Epoch  1, CatsAndDogs Batch 103:  Loss: 0.7272853255271912\n",
      "Epoch  1, CatsAndDogs Batch 104:  Loss: 0.9221531748771667\n",
      "Epoch  1, CatsAndDogs Batch 105:  Loss: 0.4458199441432953\n",
      "Epoch  1, CatsAndDogs Batch 106:  Loss: 0.6461026668548584\n",
      "Epoch  1, CatsAndDogs Batch 107:  Loss: 0.6491681933403015\n",
      "Epoch  1, CatsAndDogs Batch 108:  Loss: 0.9342803955078125\n",
      "Epoch  1, CatsAndDogs Batch 109:  Loss: 0.683246910572052\n",
      "Epoch  1, CatsAndDogs Batch 110:  Loss: 0.546324610710144\n",
      "Epoch  1, CatsAndDogs Batch 111:  Loss: 0.9167506098747253\n",
      "Epoch  1, CatsAndDogs Batch 112:  Loss: 0.6097682118415833\n",
      "Epoch  1, CatsAndDogs Batch 113:  Loss: 0.856708824634552\n",
      "Epoch  1, CatsAndDogs Batch 114:  Loss: 0.9459625482559204\n",
      "Epoch  1, CatsAndDogs Batch 115:  Loss: 0.6836304068565369\n",
      "Epoch  1, CatsAndDogs Batch 116:  Loss: 0.7626286149024963\n",
      "Epoch  1, CatsAndDogs Batch 117:  Loss: 0.677653431892395\n",
      "Epoch  1, CatsAndDogs Batch 118:  Loss: 0.679100513458252\n",
      "Epoch  1, CatsAndDogs Batch 119:  Loss: 0.5176063776016235\n",
      "Epoch  1, CatsAndDogs Batch 120:  Loss: 0.7104247212409973\n",
      "Epoch  1, CatsAndDogs Batch 121:  Loss: 0.8176000714302063\n",
      "Epoch  1, CatsAndDogs Batch 122:  Loss: 1.0244252681732178\n",
      "Epoch  1, CatsAndDogs Batch 123:  Loss: 0.8570711016654968\n",
      "Epoch  1, CatsAndDogs Batch 124:  Loss: 1.1122897863388062\n",
      "Epoch  1, CatsAndDogs Batch 125:  Loss: 0.609528660774231\n",
      "Epoch  1, CatsAndDogs Batch 126:  Loss: 0.7096139192581177\n",
      "Epoch  1, CatsAndDogs Batch 127:  Loss: 0.5395368337631226\n",
      "Epoch  1, CatsAndDogs Batch 128:  Loss: 1.084958553314209\n",
      "Epoch  1, CatsAndDogs Batch 129:  Loss: 1.5745177268981934\n",
      "Epoch  1, CatsAndDogs Batch 130:  Loss: 1.0305421352386475\n",
      "Epoch  1, CatsAndDogs Batch 131:  Loss: 1.9038517475128174\n",
      "Epoch  1, CatsAndDogs Batch 132:  Loss: 1.5843000411987305\n",
      "Epoch  1, CatsAndDogs Batch 133:  Loss: 1.2036532163619995\n",
      "Epoch  1, CatsAndDogs Batch 134:  Loss: 0.6908670663833618\n",
      "Epoch  1, CatsAndDogs Batch 135:  Loss: 1.1476141214370728\n",
      "Epoch  1, CatsAndDogs Batch 136:  Loss: 1.6976360082626343\n",
      "Epoch  1, CatsAndDogs Batch 137:  Loss: 1.007136583328247\n",
      "Epoch  1, CatsAndDogs Batch 138:  Loss: 0.6236852407455444\n",
      "Epoch  1, CatsAndDogs Batch 139:  Loss: 0.4788056015968323\n",
      "Epoch  1, CatsAndDogs Batch 140:  Loss: 0.7096473574638367\n",
      "Epoch  1, CatsAndDogs Batch 141:  Loss: 0.5492621660232544\n",
      "Epoch  1, CatsAndDogs Batch 142:  Loss: 0.8962181806564331\n",
      "Epoch  1, CatsAndDogs Batch 143:  Loss: 0.5814574956893921\n",
      "Epoch  1, CatsAndDogs Batch 144:  Loss: 0.8371938467025757\n",
      "Epoch  1, CatsAndDogs Batch 145:  Loss: 1.0933400392532349\n",
      "Epoch  1, CatsAndDogs Batch 146:  Loss: 0.8136624097824097\n",
      "Epoch  1, CatsAndDogs Batch 147:  Loss: 1.3828303813934326\n",
      "Epoch  1, CatsAndDogs Batch 148:  Loss: 0.5848745107650757\n",
      "Epoch  1, CatsAndDogs Batch 149:  Loss: 1.2817636728286743\n",
      "Epoch  1, CatsAndDogs Batch 150:  Loss: 0.6408282518386841\n",
      "Epoch  1, CatsAndDogs Batch 151:  Loss: 0.555752158164978\n",
      "Epoch  1, CatsAndDogs Batch 152:  Loss: 0.5661526918411255\n",
      "Epoch  1, CatsAndDogs Batch 153:  Loss: 0.9300373792648315\n",
      "Epoch  1, CatsAndDogs Batch 154:  Loss: 0.7205526232719421\n",
      "Epoch  1, CatsAndDogs Batch 155:  Loss: 0.8943414688110352\n",
      "Epoch  1, CatsAndDogs Batch 156:  Loss: 0.6095491051673889\n",
      "Epoch  1, CatsAndDogs Batch 157:  Loss: 0.7770853638648987\n",
      "Epoch  1, CatsAndDogs Batch 158:  Loss: 0.5226777791976929\n",
      "Epoch  1, CatsAndDogs Batch 159:  Loss: 0.7262005805969238\n",
      "Epoch  1, CatsAndDogs Batch 160:  Loss: 0.6738918423652649\n",
      "Epoch  1, CatsAndDogs Batch 161:  Loss: 0.7801569700241089\n",
      "Epoch  1, CatsAndDogs Batch 162:  Loss: 0.634740948677063\n",
      "Epoch  1, CatsAndDogs Batch 163:  Loss: 0.7948442697525024\n",
      "Epoch  1, CatsAndDogs Batch 164:  Loss: 0.6349901556968689\n",
      "Epoch  1, CatsAndDogs Batch 165:  Loss: 0.8881176114082336\n",
      "Epoch  1, CatsAndDogs Batch 166:  Loss: 0.7756074666976929\n",
      "Epoch  1, CatsAndDogs Batch 167:  Loss: 0.7526745200157166\n",
      "Epoch  1, CatsAndDogs Batch 168:  Loss: 0.7335240840911865\n",
      "Epoch  1, CatsAndDogs Batch 169:  Loss: 0.756625771522522\n",
      "Epoch  1, CatsAndDogs Batch 170:  Loss: 0.8533886075019836\n",
      "Epoch  1, CatsAndDogs Batch 171:  Loss: 0.6968498229980469\n",
      "Epoch  1, CatsAndDogs Batch 172:  Loss: 0.5259925127029419\n",
      "Epoch  1, CatsAndDogs Batch 173:  Loss: 0.6723830103874207\n",
      "Epoch  1, CatsAndDogs Batch 174:  Loss: 0.6874815225601196\n",
      "Epoch  1, CatsAndDogs Batch 175:  Loss: 0.8685334324836731\n",
      "Epoch  1, CatsAndDogs Batch 176:  Loss: 0.6465597748756409\n",
      "Epoch  1, CatsAndDogs Batch 177:  Loss: 0.6217724084854126\n",
      "Epoch  1, CatsAndDogs Batch 178:  Loss: 0.7288829684257507\n",
      "Epoch  1, CatsAndDogs Batch 179:  Loss: 0.695852518081665\n",
      "Epoch  1, CatsAndDogs Batch 180:  Loss: 0.5888770818710327\n",
      "Epoch  1, CatsAndDogs Batch 181:  Loss: 0.8254082798957825\n",
      "Epoch  1, CatsAndDogs Batch 182:  Loss: 0.6401300430297852\n",
      "Epoch  1, CatsAndDogs Batch 183:  Loss: 0.4781813621520996\n",
      "Epoch  1, CatsAndDogs Batch 184:  Loss: 0.6563151478767395\n",
      "Epoch  1, CatsAndDogs Batch 185:  Loss: 0.6497063636779785\n",
      "Epoch  1, CatsAndDogs Batch 186:  Loss: 0.8649112582206726\n",
      "Epoch  1, CatsAndDogs Batch 187:  Loss: 0.8781518936157227\n",
      "Epoch  1, CatsAndDogs Batch 188:  Loss: 0.749742865562439\n",
      "Epoch  1, CatsAndDogs Batch 189:  Loss: 0.5545707941055298\n",
      "Epoch  1, CatsAndDogs Batch 190:  Loss: 0.8292709589004517\n",
      "Epoch  1, CatsAndDogs Batch 191:  Loss: 0.6070033311843872\n",
      "Epoch  1, CatsAndDogs Batch 192:  Loss: 0.8219379186630249\n",
      "Epoch  1, CatsAndDogs Batch 193:  Loss: 0.5230266451835632\n",
      "Epoch  1, CatsAndDogs Batch 194:  Loss: 0.8286606669425964\n",
      "Epoch  1, CatsAndDogs Batch 195:  Loss: 0.5594599843025208\n",
      "Epoch  1, CatsAndDogs Batch 196:  Loss: 0.8639877438545227\n",
      "Epoch  1, CatsAndDogs Batch 197:  Loss: 0.9715360403060913\n",
      "Epoch  1, CatsAndDogs Batch 198:  Loss: 0.6477276682853699\n",
      "Epoch  1, CatsAndDogs Batch 199:  Loss: 0.9497290849685669\n",
      "Epoch  1, CatsAndDogs Batch 200:  Loss: 0.7144683003425598\n",
      "Epoch  1, CatsAndDogs Batch 201:  Loss: 0.7139980792999268\n",
      "Epoch  1, CatsAndDogs Batch 202:  Loss: 0.6075304746627808\n",
      "Epoch  1, CatsAndDogs Batch 203:  Loss: 0.7884357571601868\n",
      "Epoch  1, CatsAndDogs Batch 204:  Loss: 0.5304500460624695\n",
      "Epoch  1, CatsAndDogs Batch 205:  Loss: 0.771824061870575\n",
      "Epoch  1, CatsAndDogs Batch 206:  Loss: 0.6195484399795532\n",
      "Epoch  1, CatsAndDogs Batch 207:  Loss: 1.0730247497558594\n",
      "Epoch  1, CatsAndDogs Batch 208:  Loss: 0.5665208101272583\n",
      "Epoch  1, CatsAndDogs Batch 209:  Loss: 0.7832657694816589\n",
      "Epoch  1, CatsAndDogs Batch 210:  Loss: 0.7048047780990601\n",
      "Epoch  1, CatsAndDogs Batch 211:  Loss: 0.5544608235359192\n",
      "Epoch  1, CatsAndDogs Batch 212:  Loss: 0.5675037503242493\n",
      "Epoch  1, CatsAndDogs Batch 213:  Loss: 0.9734055399894714\n",
      "Epoch  1, CatsAndDogs Batch 214:  Loss: 0.6868510246276855\n",
      "Epoch  1, CatsAndDogs Batch 215:  Loss: 0.6371663212776184\n",
      "Epoch  1, CatsAndDogs Batch 216:  Loss: 0.6350704431533813\n",
      "Epoch  1, CatsAndDogs Batch 217:  Loss: 0.6381554007530212\n",
      "Epoch  1, CatsAndDogs Batch 218:  Loss: 0.6509904861450195\n",
      "Epoch  1, CatsAndDogs Batch 219:  Loss: 0.6097425222396851\n",
      "Epoch  1, CatsAndDogs Batch 220:  Loss: 0.6823149919509888\n",
      "Epoch  1, CatsAndDogs Batch 221:  Loss: 0.6070480346679688\n",
      "Epoch  1, CatsAndDogs Batch 222:  Loss: 0.7321087718009949\n",
      "Epoch  1, CatsAndDogs Batch 223:  Loss: 0.7087114453315735\n",
      "Epoch  1, CatsAndDogs Batch 224:  Loss: 0.6093843579292297\n",
      "Epoch  1, CatsAndDogs Batch 225:  Loss: 0.8399999737739563\n",
      "Epoch  1, CatsAndDogs Batch 226:  Loss: 0.8194435238838196\n",
      "Epoch  1, CatsAndDogs Batch 227:  Loss: 0.6408942937850952\n",
      "Epoch  1, CatsAndDogs Batch 228:  Loss: 0.6186050176620483\n",
      "Epoch  1, CatsAndDogs Batch 229:  Loss: 0.5933572053909302\n",
      "Epoch  1, CatsAndDogs Batch 230:  Loss: 0.5925256013870239\n",
      "Epoch  1, CatsAndDogs Batch 231:  Loss: 0.9911284446716309\n",
      "Epoch  1, CatsAndDogs Batch 232:  Loss: 0.6988920569419861\n",
      "Epoch  1, CatsAndDogs Batch 233:  Loss: 0.6237777471542358\n",
      "Epoch  1, CatsAndDogs Batch 234:  Loss: 0.7536746859550476\n",
      "Epoch  1, CatsAndDogs Batch 235:  Loss: 0.7490067481994629\n",
      "Epoch  1, CatsAndDogs Batch 236:  Loss: 0.6816049218177795\n",
      "Epoch  1, CatsAndDogs Batch 237:  Loss: 0.7244441509246826\n",
      "Epoch  1, CatsAndDogs Batch 238:  Loss: 0.6398038864135742\n",
      "Epoch  1, CatsAndDogs Batch 239:  Loss: 0.6797046065330505\n",
      "Epoch  1, CatsAndDogs Batch 240:  Loss: 0.7984980344772339\n",
      "Epoch  1, CatsAndDogs Batch 241:  Loss: 0.6386771202087402\n",
      "Epoch  1, CatsAndDogs Batch 242:  Loss: 0.667816162109375\n",
      "Epoch  1, CatsAndDogs Batch 243:  Loss: 0.7297573685646057\n",
      "Epoch  1, CatsAndDogs Batch 244:  Loss: 0.600037693977356\n",
      "Epoch  1, CatsAndDogs Batch 245:  Loss: 0.6635503768920898\n",
      "Epoch  1, CatsAndDogs Batch 246:  Loss: 0.7637366056442261\n",
      "Epoch  1, CatsAndDogs Batch 247:  Loss: 0.6306072473526001\n",
      "Epoch  1, CatsAndDogs Batch 248:  Loss: 0.6052088737487793\n",
      "Epoch  1, CatsAndDogs Batch 249:  Loss: 0.6286603212356567\n",
      "Epoch  1, CatsAndDogs Batch 250:  Loss: 0.72492915391922\n",
      "Epoch  1, CatsAndDogs Batch 251:  Loss: 0.756609320640564\n",
      "Epoch  1, CatsAndDogs Batch 252:  Loss: 0.7217870950698853\n",
      "Epoch  1, CatsAndDogs Batch 253:  Loss: 0.6449422836303711\n",
      "Epoch  1, CatsAndDogs Batch 254:  Loss: 0.6290432214736938\n",
      "Epoch  1, CatsAndDogs Batch 255:  Loss: 0.7075332403182983\n",
      "Epoch  1, CatsAndDogs Batch 256:  Loss: 0.47775959968566895\n",
      "Epoch  1, CatsAndDogs Batch 257:  Loss: 0.6519435048103333\n",
      "Epoch  1, CatsAndDogs Batch 258:  Loss: 0.6551901698112488\n",
      "Epoch  1, CatsAndDogs Batch 259:  Loss: 0.5827584266662598\n",
      "Epoch  1, CatsAndDogs Batch 260:  Loss: 0.653901219367981\n",
      "Epoch  1, CatsAndDogs Batch 261:  Loss: 0.8986759185791016\n",
      "Epoch  1, CatsAndDogs Batch 262:  Loss: 0.5537768006324768\n",
      "Epoch  1, CatsAndDogs Batch 263:  Loss: 0.3959531784057617\n",
      "Epoch  1, CatsAndDogs Batch 264:  Loss: 0.7017759084701538\n",
      "Epoch  1, CatsAndDogs Batch 265:  Loss: 0.6820458173751831\n",
      "Epoch  1, CatsAndDogs Batch 266:  Loss: 0.7898609042167664\n",
      "Epoch  1, CatsAndDogs Batch 267:  Loss: 0.6397421956062317\n",
      "Epoch  1, CatsAndDogs Batch 268:  Loss: 0.6869692802429199\n",
      "Epoch  1, CatsAndDogs Batch 269:  Loss: 0.8945112228393555\n",
      "Epoch  1, CatsAndDogs Batch 270:  Loss: 0.6546798944473267\n",
      "Epoch  1, CatsAndDogs Batch 271:  Loss: 0.7371869087219238\n",
      "Epoch  1, CatsAndDogs Batch 272:  Loss: 0.6168257594108582\n",
      "Epoch  1, CatsAndDogs Batch 273:  Loss: 0.7013159990310669\n",
      "Epoch  1, CatsAndDogs Batch 274:  Loss: 0.7827355265617371\n",
      "Epoch  1, CatsAndDogs Batch 275:  Loss: 0.7379915118217468\n",
      "Epoch  1, CatsAndDogs Batch 276:  Loss: 0.718600332736969\n",
      "Epoch  1, CatsAndDogs Batch 277:  Loss: 0.6058235168457031\n",
      "Epoch  1, CatsAndDogs Batch 278:  Loss: 0.6627660989761353\n",
      "Epoch  1, CatsAndDogs Batch 279:  Loss: 0.6847983598709106\n",
      "Epoch  1, CatsAndDogs Batch 280:  Loss: 0.7673026323318481\n",
      "Epoch  1, CatsAndDogs Batch 281:  Loss: 0.6933685541152954\n",
      "Epoch  1, CatsAndDogs Batch 282:  Loss: 0.6631501913070679\n",
      "Epoch  1, CatsAndDogs Batch 283:  Loss: 0.6524566411972046\n",
      "Epoch  1, CatsAndDogs Batch 284:  Loss: 0.7157849669456482\n",
      "Epoch  1, CatsAndDogs Batch 285:  Loss: 0.6996747255325317\n",
      "Epoch  1, CatsAndDogs Batch 286:  Loss: 0.705904483795166\n",
      "Epoch  1, CatsAndDogs Batch 287:  Loss: 0.73697829246521\n",
      "Epoch  1, CatsAndDogs Batch 288:  Loss: 0.6526013016700745\n",
      "Epoch  1, CatsAndDogs Batch 289:  Loss: 0.6653491854667664\n",
      "Epoch  1, CatsAndDogs Batch 290:  Loss: 0.6531649827957153\n",
      "Epoch  1, CatsAndDogs Batch 291:  Loss: 0.6829702258110046\n",
      "Epoch  1, CatsAndDogs Batch 292:  Loss: 0.6969600319862366\n",
      "Epoch  1, CatsAndDogs Batch 293:  Loss: 0.6287635564804077\n",
      "Epoch  1, CatsAndDogs Batch 294:  Loss: 0.701509416103363\n",
      "Epoch  1, CatsAndDogs Batch 295:  Loss: 0.6587816476821899\n",
      "Epoch  1, CatsAndDogs Batch 296:  Loss: 0.6881231069564819\n",
      "Epoch  1, CatsAndDogs Batch 297:  Loss: 0.6774868965148926\n",
      "Epoch  1, CatsAndDogs Batch 298:  Loss: 0.6649171113967896\n",
      "Epoch  1, CatsAndDogs Batch 299:  Loss: 0.6654442548751831\n",
      "Epoch  1, CatsAndDogs Batch 300:  Loss: 0.6674884557723999\n",
      "Epoch  1, CatsAndDogs Batch 301:  Loss: 0.6047134399414062\n",
      "Epoch  1, CatsAndDogs Batch 302:  Loss: 0.6775753498077393\n",
      "Epoch  1, CatsAndDogs Batch 303:  Loss: 0.815843939781189\n",
      "Epoch  1, CatsAndDogs Batch 304:  Loss: 0.8027976751327515\n",
      "Epoch  1, CatsAndDogs Batch 305:  Loss: 0.7135316729545593\n",
      "Epoch  1, CatsAndDogs Batch 306:  Loss: 0.5879489183425903\n",
      "Epoch  1, CatsAndDogs Batch 307:  Loss: 0.6831121444702148\n",
      "Epoch  1, CatsAndDogs Batch 308:  Loss: 0.7349430918693542\n",
      "Epoch  1, CatsAndDogs Batch 309:  Loss: 0.6339071393013\n",
      "Epoch  1, CatsAndDogs Batch 310:  Loss: 0.712527871131897\n",
      "Epoch  1, CatsAndDogs Batch 311:  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-f65b057ceae5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mtrain_neural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch {:>2}, CatsAndDogs Batch {}:  '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mprint_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Save Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-fe494232f10f>\u001b[0m in \u001b[0;36mprint_stats\u001b[0;34m(session, feature_batch, label_batch, cost, accuracy)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeature_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m#valid_acc = session.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.0})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/JorgeDeCorte/miniconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/JorgeDeCorte/miniconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/JorgeDeCorte/miniconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/JorgeDeCorte/miniconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/JorgeDeCorte/miniconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        batch_i = 0\n",
    "        for batch_x, batch_y in batches:\n",
    "            batch_i += 1\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_x, batch_y)\n",
    "            print('Epoch {:>2}, CatsAndDogs Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_x, batch_y, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
