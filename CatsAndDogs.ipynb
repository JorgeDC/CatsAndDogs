{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Jupyter notebook to distignish Cats and Dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import cv2                 # working with, mainly resizing, images\n",
    "import numpy as np         # dealing with arrays\n",
    "import os                  # dealing with directories\n",
    "from random import shuffle # mixing up or currently ordered data that might lead our network astray in training.\n",
    "from tqdm import tqdm      # a nice pretty percentage bar for tasks. Thanks to viewer Daniel BA1/4hler for this suggestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DIR = 'train'\n",
    "TEST_DIR = 'test'\n",
    "IMG_SIZE = 50\n",
    "LR = 1e-3\n",
    "\n",
    "print(\"done loading\")\n",
    "\n",
    "MODEL_NAME = 'dogsvscats-{}-{}.model'.format(LR, '2conv-basic') # just so we remember which saved model is which, sizes must match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_img(img):\n",
    "    word_label = img.split('.')[-3]\n",
    "    # conversion to one-hot array [cat,dog]\n",
    "    #                            [much cat, no dog]\n",
    "    if word_label == 'cat': return [1,0]\n",
    "    #                             [no cat, very doggo]\n",
    "    elif word_label == 'dog': return [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_train_data():\n",
    "    training_data = []\n",
    "    for img in tqdm(os.listdir(TRAIN_DIR)):\n",
    "        if img[-4:] == \".jpg\":\n",
    "            label = label_img(img)\n",
    "            path = os.path.join(TRAIN_DIR,img)\n",
    "            img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "            training_data.append([np.array(img),np.array(label)])\n",
    "    shuffle(training_data)\n",
    "    np.save('train_data.npy', training_data)\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_test_data():\n",
    "    testing_data = []\n",
    "    for img in tqdm(os.listdir(TEST_DIR)):\n",
    "        path = os.path.join(TEST_DIR,img)\n",
    "        img_num = img.split('.')[0]\n",
    "        img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "        testing_data.append([np.array(img), img_num])\n",
    "        \n",
    "    shuffle(testing_data)\n",
    "    np.save('test_data.npy', testing_data)\n",
    "    print(saved)\n",
    "    return testing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_data = create_train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading trainig data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = np.load('train_data.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_net_image_input(image_shape):\n",
    "    return tf.placeholder(tf.float32, shape=[None, image_shape[0], image_shape[1], image_shape[2]], name=\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_net_label_input(n_classes):\n",
    "    return tf.placeholder(tf.float32, shape=[None, n_classes], name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_net_keep_prob_input():\n",
    "    return tf.placeholder(tf.float32, name=\"keep_prob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    depth = int(x_tensor.shape[3])\n",
    "    filter_weights = tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1],depth, conv_num_outputs], stddev=0.1))\n",
    "    filter_bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    stridesTuple = [1, conv_strides[0], conv_strides[1], 1] # (batch, height, width, depth)\n",
    "    paddingString = 'SAME'\n",
    "    conv = tf.nn.conv2d(x_tensor, filter_weights, strides=stridesTuple, padding=paddingString)\n",
    "    conv = tf.nn.bias_add(conv, filter_bias)\n",
    "    conv = tf.nn.relu(conv)\n",
    "    poolTuple = [1, pool_strides[0], pool_strides[1], 1] # (batch, height, width, depth)\n",
    "    conv = tf.nn.max_pool(conv,ksize=[1, pool_ksize[0], pool_ksize[1], 1],strides=poolTuple,padding=paddingString)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(x_tensor):\n",
    "    return tf.contrib.layers.flatten(x_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    return tf.contrib.layers.fully_connected(inputs=x_tensor, num_outputs=num_outputs, activation_fn=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    return tf.contrib.layers.fully_connected(inputs=x_tensor, num_outputs=num_outputs, activation_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    conv = conv2d_maxpool(x, 16, (8,8), (1,1), (2,2), (2,2))\n",
    "    conv = conv2d_maxpool(x, 32, (4,4), (1,1), (2,2), (2,2))\n",
    "    conv = conv2d_maxpool(x, 64, (2,2), (1,1), (2,2), (2,2))\n",
    "\n",
    "    conv = flatten(conv)\n",
    "    \n",
    "    conv = fully_conn(conv, 200)\n",
    "    conv = tf.nn.dropout(conv, keep_prob)\n",
    "    conv = fully_conn(conv, 100)\n",
    "    conv = tf.nn.dropout(conv, keep_prob)\n",
    "    \n",
    "\n",
    "    conv = output(conv, 2)\n",
    "    \n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((IMG_SIZE, IMG_SIZE, 1))\n",
    "y = neural_net_label_input(2)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    output = session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "batch_size = 1000\n",
    "keep_probability = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train_data[:-2000]\n",
    "test = train_data[-2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = np.array([i[0] for i in train]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "train_y = [i[1] for i in train]\n",
    "\n",
    "test_x = np.array([i[0] for i in test]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "test_y = [i[1] for i in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    loss = session.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.0})\n",
    "    valid_acc = session.run(accuracy, feed_dict={x: test_x, y: test_y, keep_prob: 1.0})\n",
    "    print('Loss: {} Validation Accuracy: {}'.format(loss, valid_acc))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chunks(x, y, n):\n",
    "    for i in range(0, len(x), n):\n",
    "        yield x[i:i + n], y[i:i + n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batches = chunks(train_x, train_y, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, CatsAndDogs Batch 1:  Loss: 941.526611328125 Validation Accuracy: 0.48350000381469727\n",
      "Epoch  1, CatsAndDogs Batch 2:  Loss: 368.46148681640625 Validation Accuracy: 0.48350000381469727\n",
      "Epoch  1, CatsAndDogs Batch 3:  Loss: 41.37310028076172 Validation Accuracy: 0.5164999961853027\n",
      "Epoch  1, CatsAndDogs Batch 4:  Loss: 44.25331115722656 Validation Accuracy: 0.48350000381469727\n",
      "Epoch  1, CatsAndDogs Batch 5:  Loss: 45.05410385131836 Validation Accuracy: 0.5164999961853027\n",
      "Epoch  1, CatsAndDogs Batch 6:  Loss: 38.332218170166016 Validation Accuracy: 0.48350000381469727\n",
      "Epoch  1, CatsAndDogs Batch 7:  Loss: 19.805578231811523 Validation Accuracy: 0.5164999961853027\n",
      "Epoch  1, CatsAndDogs Batch 8:  Loss: 32.382781982421875 Validation Accuracy: 0.48350000381469727\n",
      "Epoch  1, CatsAndDogs Batch 9:  Loss: 16.041141510009766 Validation Accuracy: 0.5164999961853027\n",
      "Epoch  1, CatsAndDogs Batch 10:  Loss: 28.20500373840332 Validation Accuracy: 0.48350000381469727\n",
      "Epoch  1, CatsAndDogs Batch 11:  Loss: 5.281888961791992 Validation Accuracy: 0.5210000276565552\n",
      "Epoch  1, CatsAndDogs Batch 12:  Loss: 16.63345718383789 Validation Accuracy: 0.48350000381469727\n",
      "Epoch  1, CatsAndDogs Batch 13:  Loss: 24.365446090698242 Validation Accuracy: 0.515999972820282\n",
      "Epoch  1, CatsAndDogs Batch 14:  Loss: 4.377020359039307 Validation Accuracy: 0.5644999742507935\n",
      "Epoch  1, CatsAndDogs Batch 15:  Loss: 3.6278598308563232 Validation Accuracy: 0.5755000114440918\n",
      "Epoch  1, CatsAndDogs Batch 16:  Loss: 3.763571262359619 Validation Accuracy: 0.5540000200271606\n",
      "Epoch  1, CatsAndDogs Batch 17:  Loss: 19.37108612060547 Validation Accuracy: 0.48350000381469727\n",
      "Epoch  1, CatsAndDogs Batch 18:  Loss: 12.568784713745117 Validation Accuracy: 0.5164999961853027\n",
      "Epoch  1, CatsAndDogs Batch 19:  Loss: 8.245139122009277 Validation Accuracy: 0.48350000381469727\n",
      "Epoch  1, CatsAndDogs Batch 20:  Loss: 18.877988815307617 Validation Accuracy: 0.5164999961853027\n",
      "Epoch  1, CatsAndDogs Batch 21:  Loss: 1.7563426494598389 Validation Accuracy: 0.578499972820282\n",
      "Epoch  1, CatsAndDogs Batch 22:  Loss: 29.06418800354004 Validation Accuracy: 0.48350000381469727\n",
      "Epoch  1, CatsAndDogs Batch 23:  Loss: 14.263050079345703 Validation Accuracy: 0.48350000381469727\n"
     ]
    }
   ],
   "source": [
    "save_model_path = './catsvsdogs_classification'\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        batch_i = 0\n",
    "        for batch_x, batch_y in batches:\n",
    "            batch_i += 1\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_x, batch_y)\n",
    "            print('Epoch {:>2}, CatsAndDogs Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_x, batch_y, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
